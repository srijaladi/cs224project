import torchimport openaiimport reimport numpy as npimport randomimport matplotlibfrom matplotlib import pyplot as pltimport requestsimport osimport transformersfrom transformers import GPT2Config, GPT2Modelimport randomimport bisectfrom bisect import bisect_left, bisect_rightfrom transformers import Trainerimport timefrom string import punctuationfrom sentence_transformers import SentenceTransformerfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizerfrom transformers import pipeline from torch import nn as nnDEVICE = "cuda" if torch.cuda.is_available() else "cpu"MODEL_NAME = 'bert-base-uncased'MODEL_IS_GPT = MODEL_NAME[:4] == 'gpt2'EMBED_TOOL = "bert"COHERENCE_TOOL = "gpt3"HYPERPARAMETERS = {'alpha': 1, 'beta': 4, 'delta': 3}TOP_K_WORDS = 100LEARNING_RATE = 0.1NUM_MASK = int(3)MASK_CUTOFF = NoneCOMPLETED_FILEPATH = "completed_sents.txt"MODEL_PARAMS_FILEPATH = "modelParamsNetwork/params_" + str(TOP_K_WORDS)TRAIN_LOSS_FILEPATH = "train_loss_vals.txt"print("STARTED RUNNING")SENTENCE_EMBED_TOOL = SentenceTransformer('all-MiniLM-L6-v2')BERT_RESPONSE = None #pipeline("text-generation", model="BertLMHeadModel")GPT2_RESPONSE = pipeline('text-generation', model = 'gpt2')Cls = transformers.AutoModelForCausalLMBASE_MODEL = Cls.from_pretrained(MODEL_NAME)if isinstance(BASE_MODEL, transformers.GPT2LMHeadModel):    BASE_MODEL.transformer.gradient_checkpointing_enable()BASE_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)if BASE_TOKENIZER.pad_token_id is None:    if Cls == transformers.AutoModelForCausalLM:        BASE_TOKENIZER.pad_token = BASE_TOKENIZER.eos_token    else:        print("Adding pad token to tokenizer")        BASE_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})        BASE_TOKENIZER.pad_token = '[PAD]'        FT_MODEL = Cls.from_pretrained(MODEL_NAME)if isinstance(FT_MODEL, transformers.GPT2LMHeadModel):    FT_MODEL.transformer.gradient_checkpointing_enable()TOKENIZER = FT_TOKENIZER = BASE_TOKENIZERGPTZERO_API_URL = "https://api.gptzero.me/v2/predict/text"todo = {"document": "I am running to the gym."}response = requests.post(GPTZERO_API_URL, json=todo)response.json()API_KEY = "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"openai.api_key = API_KEYMODEL_ENGINE = "text-davinci-003"FOLDER_PATH = "data/"KEYS_PATH = "keys.txt"def compute_sentences(responses):    essays = [i.split("\n") for i in responses]    sentences = []    for essay_li in essays:        essay_sents = []        for portion in essay_li:            if len(portion.strip()) == 0:                continue            add_li = re.split('(?<=[.!?]) +',str(portion))            essay_sents += add_li            #print(essay_sents)        sentences.append(essay_sents)    return sentences    def compute_sentences_single_essay(essay):    essay_li = essay.split("\n")    essay_sents = []    for portion in essay_li:        if len(portion.strip()) == 0:            continue        add_li = re.split('(?<=[.!?]) +',str(portion))        essay_sents += add_li    return essay_sentsdef sentence_embedding(input_sentence, version = "bert", return_type = "torch"):        if version == "openai" or version == "gpt":        response = openai.Embedding.create(        input=input_sentence,        engine="text-similarity-davinci-001")        res = response.data[0]['embedding']                if return_type.lower() == "np" or return_type.lower() == "numpy":            return np.array(res)        elif return_type.lower() == "list":            return res        else:            return torch.tensor(res)    else:        res = SENTENCE_EMBED_TOOL.encode([input_sentence])[0]        if return_type.lower() == "np" or return_type.lower() == "numpy":            return np.array(res)        elif return_type.lower() == "list":            return list(res)        else:            return torch.tensor(res)    def similarity_score_single(sentence1, sentence2):    embed1 = sentence_embedding(sentence1, EMBED_TOOL, "torch")    embed2 = sentence_embedding(sentence2, EMBED_TOOL, "torch")    norm1 = torch.sqrt(torch.sum(embed1 * embed1))    norm2 = torch.sqrt(torch.sum(embed2 * embed2))    numerator = torch.dot(embed1, embed2)    denominator = norm1 * norm2    return numerator/denominatordef sentence_coherence_score_single(input_sentence, version = "gpt2"):    return 0.5    if version == "openai" or version == "gpt3":        modified_prompt = "Evaluate the coherence score of this sentence as a value between 0 and 1:\n\n" + input_sentence        response = openai.Completion.create(          model=MODEL_ENGINE,          prompt=modified_prompt,          temperature=0,          max_tokens=60,          top_p=1.0,          frequency_penalty=0.0,          presence_penalty=0.0        )        res = response.choices[0]['text'].strip()        return float(res)    elif version == "gpt2":        modified_prompt = "Evaluate the coherence score of this sentence as a single value between 0 and 1: " + input_sentence        res = GPT2_RESPONSE(modified_prompt, max_length = 1000, num_return_sequences=1)[0]['generated_text']        print(res.split())        res = res.split(" ")[-1]        print(res)        return float(res)    else:        modified_prompt = "Evaluate the coherence score of this sentence as a single value between 0 and 1: " + input_sentence        res = BERT_RESPONSE(modified_prompt, max_length = 5, num_return_sequences=1)[0]['generated_text']        res = res.split(" ")[-1]        print(res)        return float(res)        def get_prob(model, tokenizer, full_sentence, encoded_sentence):        isGPT = MODEL_NAME[:4] == 'gpt2'    def get_word_prob(ids_so_far, true_token):        with torch.inference_mode():            if len(ids_so_far.size()) == 1 and not(isGPT):                ids_so_far = torch.unsqueeze(ids_so_far,0)            end_model = model(input_ids = ids_so_far)            logits = end_model.logits            all_probs = torch.nn.functional.softmax(logits, dim = -1)                        if isGPT: probability = all_probs[-1][true_token]            else: probability =  all_probs[0][-1][true_token]                            return probability        all_probs = torch.zeros(len(encoded_sentence))    total_log_prob = 0    #print(all_probs)    for i in range(0,len(encoded_sentence)):        word_cond_prob = get_word_prob(encoded_sentence[:i+1], encoded_sentence[i])        all_probs[i] = word_cond_prob        total_log_prob += np.log(word_cond_prob)        return total_log_prob, all_probsdef compute_perplexity(full_sentence, encoded_sentence):    base_log_prob, base_each_prob = get_prob(BASE_MODEL, TOKENIZER, full_sentence, encoded_sentence)    #print(base_log_prob)    N = len(encoded_sentence)        overall_perplexity = 2 ** (-(1/N) * base_log_prob/np.log(2)) #(1/base_prob) ** (1/len(encoded_sentence))    #print(overall_perplexity)    return overall_perplexity, base_each_probdef compute_loss(new_sentence, original_sentence):    a = HYPERPARAMETERS['alpha']    b = HYPERPARAMETERS['beta']    d = HYPERPARAMETERS['delta']        new_encoded_sentence = TOKENIZER(new_sentence, return_tensors = 'pt')['input_ids'][0]        perplexity, _ = compute_perplexity(new_sentence, new_encoded_sentence)    coherence = sentence_coherence_score_single(new_sentence, COHERENCE_TOOL)    similarity = similarity_score_single(new_sentence, original_sentence)        objective = a * perplexity + b * coherence + d * similarity    loss = -objective        return loss, perplexity, coherence, similarity    def find_mask_indexes(full_sentence, encoded_sentence, num_mask = None, mask_cutoff = None):    sentence_perplexity, prob_each_index = compute_perplexity(full_sentence, encoded_sentence)    indexes_by_prob = [[p,i] for i,p in enumerate(prob_each_index) if i > 0 and i < len(prob_each_index)-1]    indexes_by_prob = sorted(indexes_by_prob)        #print(indexes_by_prob)        if not(num_mask is None):        res = [tu[1] for tu in indexes_by_prob[:num_mask]]    elif not(mask_cutoff is None):        res = []        for p,i in indexes_by_prob:            if p < mask_cutoff:                break            res.append(i)    else:        print("ERROR: NEED TYPE OF MASK (EITHER NUMBER OR CUTOFF)")        return None        return resdef fill_masked_indexes(ft_model, ft_tokenizer, sentence, encoded_sentence, mask_indexes):    def get_inference(all_ids, idx):        isGPT = MODEL_NAME[:4] == 'gpt2'        with torch.inference_mode():              if len(all_ids.size()) == 1 and not(isGPT):                all_ids = torch.unsqueeze(all_ids,0)            end_model = ft_model(input_ids = all_ids)            logits = end_model.logits            if isGPT: res = torch.argmax(logits[idx])            else: res = torch.topk(logits[0][idx],2)[1][0]            print(res)            return res        curr_encoded_sentence = torch.clone(encoded_sentence)    for idx in mask_indexes:        new_token = get_inference(curr_encoded_sentence, idx)        curr_encoded_sentence[idx] = new_token        return curr_encoded_sentencedef modify_sentence_full(sentence: str, num_mask = None, mask_cutoff = None, printing = False):    encoded_sentence = BASE_TOKENIZER(sentence, return_tensors = 'pt')['input_ids'][0]    mask_indexes = find_mask_indexes(sentence, encoded_sentence, num_mask = num_mask, mask_cutoff = mask_cutoff)        final_sentence_encoded_full = fill_masked_indexes(FT_MODEL, FT_TOKENIZER, sentence, encoded_sentence, mask_indexes)    final_sentence_encoded_chop = final_sentence_encoded_full[1:-1]    final_sentence_decoded_chop = FT_TOKENIZER.decode(final_sentence_encoded_chop)    final_sentence_decoded = final_sentence_decoded_chop.lstrip(punctuation).strip()        if printing:        print(sentence)        print("Tokenized Sentence: ", encoded_sentence)        print("Masked Indices in order", mask_indexes)        print([FT_TOKENIZER.decode(encoded_sentence[index]) for index in mask_indexes])        print("Final Sentence Tokenized:", final_sentence_encoded_chop)        print("Decoded Final Sentence: ", final_sentence_decoded)            return final_sentence_decodeddef network_modify_sentence_full(network, sentence: str, num_mask = None, mask_cutoff = None):    original_sentence = sentence    encoded_sentence = TOKENIZER(original_sentence, return_tensors = 'pt')['input_ids'][0]        masked_indexes = find_mask_indexes(sentence, encoded_sentence, num_mask = num_mask, mask_cutoff = mask_cutoff)        for mask_index in masked_indexes:        curr_encoded_sentence = torch.clone(encoded_sentence)               token_dist = get_token_dist(curr_encoded_sentence, mask_index)                sorted_dist, corres_tokens = torch.sort(token_dist, descending = True)        sorted_dist = sorted_dist[:TOP_K_WORDS]        corres_tokens = corres_tokens[:TOP_K_WORDS]                output_values = network(sorted_dist)                best_token = corres_tokens[torch.argmax(output_values)]                curr_encoded_sentence[mask_index] = best_token        original_sentence = full_tokens_to_sentence(curr_encoded_sentence)        encoded_sentence = curr_encoded_sentence        final_sentence_decoded = full_tokens_to_sentence(encoded_sentence)            return final_sentence_decodeddef full_tokens_to_sentence(sentence_all_tokens):    final_sentence_encoded_chop = sentence_all_tokens[1:-1]    final_sentence_decoded_chop = TOKENIZER.decode(final_sentence_encoded_chop)    final_sentence_decoded = final_sentence_decoded_chop.lstrip(punctuation).strip()    return final_sentence_decodeddef get_token_dist(curr_encoded_sentence, token_index):    with torch.inference_mode():        if len(curr_encoded_sentence.size()) == 1 and not(MODEL_IS_GPT):            curr_encoded_sentence = torch.unsqueeze(curr_encoded_sentence,0)        end_model = BASE_MODEL(input_ids = curr_encoded_sentence)        logits = end_model.logits        all_probs = torch.nn.functional.softmax(logits, dim = -1)                if MODEL_IS_GPT: token_dist = all_probs[token_index]        else: token_dist =  all_probs[0][token_index]                    return token_dist    def train_on_sentence(network, origin_sentence, num_mask, mask_cutoff):    original_sentence = origin_sentence    encoded_sentence = TOKENIZER(original_sentence, return_tensors = 'pt')['input_ids'][0]    #print(original_sentence)    #print(encoded_sentence)    masked_indexes = find_mask_indexes(original_sentence, encoded_sentence, num_mask = num_mask, mask_cutoff = mask_cutoff)    for mask_index in masked_indexes:        curr_encoded_sentence = torch.clone(encoded_sentence)        old_loss = compute_loss(original_sentence, original_sentence)[0]               token_dist = get_token_dist(curr_encoded_sentence, mask_index)                sorted_dist, corres_tokens = torch.sort(token_dist, descending = True)        sorted_dist = sorted_dist[:TOP_K_WORDS]        corres_tokens = corres_tokens[:TOP_K_WORDS]                true_loss_ratios = []        chosen_indexes = []                #print("BEFORE: ", network(sorted_dist))                range_amt = random.randint(5,6)        for chosen_index in range(range_amt):            curr_token = corres_tokens[chosen_index]            curr_encoded_sentence[mask_index] = curr_token            decoded_new_sentence = full_tokens_to_sentence(curr_encoded_sentence)                        new_loss = compute_loss(decoded_new_sentence, original_sentence)[0]            print("loss:", new_loss)            old_loss = old_loss            loss_ratio = new_loss/old_loss                        true_loss_ratios.append(loss_ratio)            chosen_indexes.append(chosen_index)                    network.backprop_batch_single_input(sorted_dist, chosen_indexes, true_loss_ratios)                best_token = corres_tokens[np.argmax(np.array(true_loss_ratios))]        curr_encoded_sentence[mask_index] = best_token        original_sentence = full_tokens_to_sentence(curr_encoded_sentence)        encoded_sentence = curr_encoded_sentence        post_sentence = network_modify_sentence_full(network, origin_sentence, num_mask, mask_cutoff)    sent_loss_post = compute_loss(post_sentence, origin_sentence)[0]        return sent_loss_post.item()    class modelTrainer(Trainer):    def compute_loss(self, model, input_sentence_tokens, mask_index, chosen_token, computed_loss):                if len(input_sentence_tokens.size()) == 1 and not(MODEL_IS_GPT):            input_sentence_tokens = torch.unsqueeze(input_sentence_tokens,0)                    end_model = model(input_ids = input_sentence_tokens)        logits = end_model.logits        all_probs = torch.nn.functional.softmax(logits, dim = -1)                if MODEL_IS_GPT: probability = all_probs[mask_index][chosen_token]        else: probability =  all_probs[0][mask_index][chosen_token]                    loss = probability * compute_loss        return loss    class HeadNetwork(nn.Module):    def __init__(self, input_size):        super().__init__()        layers = [nn.Linear(input_size, input_size * 4),                   nn.ReLU(),                   nn.Linear(input_size * 4, input_size * 2),                   nn.ReLU(),                   nn.Linear(input_size * 2, input_size)                  ]                self.input_size = input_size        self.model = nn.Sequential(*layers)        self.model = self.model.to(DEVICE)        self.learning_rate = LEARNING_RATE        self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = self.learning_rate)            def forward(self, input_logits):        return self.model(input_logits)        def backprop_single(self, input_logits, chosen_index, target):        self.optimizer.zero_grad()        output_logits = self.model(input_logits)                assert output_logits.size(dim = 0) == self.input_size                loss = (output_logits[chosen_index] - target) ** 2        loss.backward()        self.optimizer.step()        return loss.item()        def backprop_batch_single_input(self, input_logits, chosen_index_list, target_vals):        self.optimizer.zero_grad()        output_logits = self.model(input_logits)                assert output_logits.size(dim = 0) == self.input_size                chosen_logits = output_logits[chosen_index_list]                mse_loss = torch.mean(torch.square((chosen_logits - torch.tensor(target_vals))))        mse_loss.backward()        self.optimizer.step()        return mse_loss.item()        def backprop_batch(self, batched_input_logits, chosen_index_list, target_vals):        self.optimizer.zero_grad()                output_logits = self.model(batched_input_logits)                assert output_logits.size(dim = 0) == batched_input_logits.size(dim = 0)        assert output_logits.size(dim = 1) == self.input_size        assert output_logits.size(dim = 0) == len(chosen_index_list)        assert len(chosen_index_list) == len(target_vals)                mask = torch.nn.functional.one_hot(torch.tensor(chosen_index_list).to(torch.int64), self.input_size)                output_values = torch.sum(mask * output_logits, dim = 1)            mse_loss = torch.mean(torch.square(output_values - torch.tensor(target_vals)))        mse_loss.backward()        self.optimizer.step()        return mse_loss.item()    def save_model_info(model, completed_count, train_loss_vals):    model_filepath = MODEL_PARAMS_FILEPATH + "_" + str(completed_count) + ".txt"    torch.save(model.state_dict(), model_filepath)    text_file = open(COMPLETED_FILEPATH, "w")    text_file.write(str(completed_count))    text_file.close()    np.savetxt(TRAIN_LOSS_FILEPATH, train_loss_vals)    network = HeadNetwork(TOP_K_WORDS)completed_so_far = 0with open('completed_sents.txt') as f:    lines = f.readlines()[0]    completed_so_far = int(lines.strip())    completed_so_far -= completed_so_far%10completed_so_far = int(completed_so_far)if completed_so_far > 0:    network.load_state_dict(torch.load(MODEL_PARAMS_FILEPATH + "_" + str(completed_so_far) + ".txt"))        all_train_sents = []with open('train_sents.txt') as f:    lines = f.readlines()    all_train_sents = lines    train_loss_vals = np.loadtxt(TRAIN_LOSS_FILEPATH)    print()print("Training Started From " + str(completed_so_far))print()for curr_sent_index in range(completed_so_far, len(all_train_sents)):    curr_sentence = all_train_sents[curr_sent_index].strip()    completed_so_far += 1        print(curr_sentence)    if len(curr_sentence) >= 10:        curr_loss = train_on_sentence(network, curr_sentence, NUM_MASK, MASK_CUTOFF)        train_loss_vals = np.append(train_loss_vals, curr_loss)        print("Loss for completion " + str(completed_so_far) + ": " + str(curr_loss))    else:        print("Skipped completion " + str(completed_so_far))        if completed_so_far%10 == 0:        save_model_info(network, completed_so_far, train_loss_vals)                all_dev_sents = []with open('dev_sents.txt') as f:    lines = f.readlines()    all_dev_sents = lines.split("\n")    all_test_sents = []with open('test_sents.txt') as f:    lines = f.readlines()    all_test_sents = lines.split("\n")