{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e8910d-b302-41a6-ae8d-2b36cdcc0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import random\n",
    "import bisect\n",
    "from bisect import bisect_left, bisect_right\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b24a14-d998-4087-b6b2-0f97cbc91d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'average_generated_prob': 0,\n",
       "   'completely_generated_prob': 0.11111111111111108,\n",
       "   'overall_burstiness': 0,\n",
       "   'paragraphs': [{'completely_generated_prob': 0.11111111111111108,\n",
       "     'num_sentences': 1,\n",
       "     'start_sentence_index': 0}],\n",
       "   'sentences': [{'generated_prob': 0,\n",
       "     'perplexity': 92,\n",
       "     'sentence': 'I am running to the gym.'}]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTZERO_API_URL = \"https://api.gptzero.me/v2/predict/text\"\n",
    "todo = {\"document\": \"I am running to the gym.\"}\n",
    "response = requests.post(GPTZERO_API_URL, json=todo)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3824db4-f904-4b78-a930-988c926d3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"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\"\n",
    "openai.api_key = API_KEY\n",
    "MODEL_ENGINE = \"text-davinci-003\"\n",
    "FOLDER_PATH = \"data/\"\n",
    "KEYS_PATH = \"keys.txt\"\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "Cls = transformers.AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = Cls.from_pretrained(MODEL_NAME)\n",
    "if isinstance(BASE_MODEL, transformers.GPT2LMHeadModel):\n",
    "    BASE_MODEL.transformer.gradient_checkpointing_enable()\n",
    "BASE_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if BASE_TOKENIZER.pad_token_id is None:\n",
    "    if Cls == transformers.AutoModelForCausalLM:\n",
    "        BASE_TOKENIZER.pad_token = BASE_TOKENIZER.eos_token\n",
    "    else:\n",
    "        print(\"Adding pad token to tokenizer\")\n",
    "        BASE_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        BASE_TOKENIZER.pad_token = '[PAD]'\n",
    "        \n",
    "FT_MODEL = Cls.from_pretrained(MODEL_NAME)\n",
    "if isinstance(FT_MODEL, transformers.GPT2LMHeadModel):\n",
    "    FT_MODEL.transformer.gradient_checkpointing_enable()\n",
    "FT_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if FT_TOKENIZER.pad_token_id is None:\n",
    "    if Cls == transformers.AutoModelForCausalLM:\n",
    "        FT_TOKENIZER.pad_token = FT_TOKENIZER.eos_token\n",
    "    else:\n",
    "        print(\"Adding pad token to tokenizer\")\n",
    "        FT_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        FT_TOKENIZER.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c90783d-78cc-4ac0-aa43-2240d03f3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_coherence(sentence):\n",
    "    modified_prompt = \"\"\"answer in one word yes or no: does this make sense as a sentence \\\"\"\"\" + sentence + \"\"\"\\\"\"\"\"\n",
    "    print(modified_prompt)\n",
    "    # Generate a response\n",
    "    completion = openai.Completion.create(\n",
    "        engine=MODEL_ENGINE,\n",
    "        prompt=modified_prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].text.strip()\n",
    "    print(res)\n",
    "    if res.lower()[:2] == \"no\":\n",
    "        return \"Incoherent\"\n",
    "    elif res.lower()[:3] == \"yes\":\n",
    "        return \"Coherent\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def sentence_embedding(input_sentence, return_type = \"torch\"):\n",
    "    response = openai.Embedding.create(\n",
    "    input=input_sentence,\n",
    "    engine=\"text-similarity-davinci-001\")\n",
    "    res = response.data[0]['embedding']\n",
    "    \n",
    "    if return_type.lower() == \"np\" or return_type.lower() == \"numpy\":\n",
    "        return np.array(res)\n",
    "    elif return_type.lower() == \"list\":\n",
    "        return res\n",
    "    else:\n",
    "        return torch.tensor(res)\n",
    "    \n",
    "def similarity_score_single(sentence1, sentence2):\n",
    "    embed1 = sentence_embedding(sentence1, \"torch\")\n",
    "    embed2 = sentence_embedding(sentence2, \"torch\")\n",
    "    norm1 = torch.sqrt(torch.sum(embed1 * embed1))\n",
    "    norm2 = torch.sqrt(torch.sum(embed2 * embed2))\n",
    "    numerator = torch.dot(embed1, embed2)\n",
    "    denominator = norm1 * norm2\n",
    "    return numerator/denominator\n",
    "\n",
    "def sentence_coherence_score_single(input_sentence):\n",
    "    modified_prompt = \"Evaluate the coherence score of this sentence as a value between 0 and 1:\\n\\n\" + input_sentence\n",
    "    response = openai.Completion.create(\n",
    "      model=MODEL_ENGINE,\n",
    "      prompt=modified_prompt,\n",
    "      temperature=0,\n",
    "      max_tokens=60,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0\n",
    "    )\n",
    "    res = response.choices[0]['text'].strip()\n",
    "    return float(res)\n",
    "\n",
    "def compute_sentences(responses):\n",
    "    essays = [i.split(\"\\n\") for i in responses]\n",
    "    sentences = []\n",
    "    for essay_li in essays:\n",
    "        essay_sents = []\n",
    "        for portion in essay_li:\n",
    "            if len(portion.strip()) == 0:\n",
    "                continue\n",
    "            add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "            essay_sents += add_li\n",
    "            #print(essay_sents)\n",
    "        sentences.append(essay_sents)\n",
    "    return sentences\n",
    "    \n",
    "def compute_sentences_single_essay(essay):\n",
    "    essay_li = essay.split(\"\\n\")\n",
    "    essay_sents = []\n",
    "    for portion in essay_li:\n",
    "        if len(portion.strip()) == 0:\n",
    "            continue\n",
    "        add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "        essay_sents += add_li\n",
    "    return essay_sents\n",
    "\n",
    "def collect_data(word):\n",
    "    num = 3\n",
    "    prompt = \"Write a long essay about \" + word\n",
    "    completion = openai.Completion.create(\n",
    "        engine=MODEL_ENGINE,\n",
    "        prompt=prompt,\n",
    "        max_tokens=3500,\n",
    "        n=num,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "        \n",
    "    responses = [completion.choices[i].text for i in range(len(completion.choices))]\n",
    "    \n",
    "    sentences_per = compute_sentences(responses)\n",
    "    lens = [len(sen) for sen in sentences_per]\n",
    "    \n",
    "    for i,essay in enumerate(responses):\n",
    "        filepath = FOLDER_PATH + word + str(i)\n",
    "        f = open(filepath, \"w\")\n",
    "        f.write(essay)\n",
    "        f.close()\n",
    "        \n",
    "    return lens\n",
    "        \n",
    "def gen_data(num_words):\n",
    "    all_nouns = []\n",
    "    file1 = open('nounlist.txt', 'r')\n",
    "    lines = file1.readlines()\n",
    "    all_nouns = [i.strip() for i in lines]\n",
    "    \n",
    "    amt_keys = np.loadtxt(KEYS_PATH)\n",
    "    prev_gen = np.sum(amt_keys)\n",
    "        \n",
    "    start_index = int(len(amt_keys)/3)\n",
    "    stop_index = min(start_index + num_words, len(all_nouns))\n",
    "    for i in range(start_index,stop_index):\n",
    "        print(i)\n",
    "        lens = collect_data(all_nouns[i])\n",
    "        \n",
    "        amt_keys = np.append(amt_keys, np.array(lens))\n",
    "        total_gen = np.sum(amt_keys)\n",
    "        \n",
    "        print(\"Generated: \" + str(lens) + \" for: \" + str(all_nouns[i]))\n",
    "        print(\"Total generated now: \" + str(total_gen) + \", Generated this iteration: \" + str(total_gen - prev_gen))\n",
    "        \n",
    "        np.savetxt(KEYS_PATH, amt_keys)\n",
    "        \n",
    "        time.sleep(60)\n",
    "        \n",
    "def get_prob(model, tokenizer, full_sentence, encoded_sentence):    \n",
    "    isGPT = MODEL_NAME[:4] == 'gpt2'\n",
    "    def get_word_prob(ids_so_far, true_token):\n",
    "        with torch.inference_mode():\n",
    "            if len(ids_so_far.size()) == 1 and not(isGPT):\n",
    "                ids_so_far = torch.unsqueeze(ids_so_far,0)\n",
    "            end_model = model(input_ids = ids_so_far)\n",
    "            logits = end_model.logits\n",
    "            #print(ids_so_far)\n",
    "            #print(tokenizer.decode(ids_so_far))\n",
    "            #print(logits.size())\n",
    "            all_probs = torch.nn.functional.softmax(logits, dim = -1)\n",
    "            \n",
    "            if isGPT: probability = all_probs[-1][true_token]\n",
    "            else: probability =  all_probs[0][-1][true_token]\n",
    "                \n",
    "            return probability\n",
    "    \n",
    "    all_probs = torch.zeros(len(encoded_sentence))\n",
    "\n",
    "    total_log_prob = 0\n",
    "    #print(all_probs)\n",
    "    for i in range(0,len(encoded_sentence)):\n",
    "        word_cond_prob = get_word_prob(encoded_sentence[:i+1], encoded_sentence[i])\n",
    "        all_probs[i] = word_cond_prob\n",
    "        total_log_prob += np.log(word_cond_prob)\n",
    "    \n",
    "    return total_log_prob, all_probs\n",
    "\n",
    "def compute_perplexity(model, tokenizer, full_sentence, encoded_sentence):\n",
    "    base_log_prob, base_each_prob = get_prob(model, tokenizer, full_sentence, encoded_sentence)\n",
    "    #print(base_log_prob)\n",
    "    N = len(encoded_sentence)\n",
    "    \n",
    "    overall_perplexity = 2 ** (-(1/N) * base_log_prob/np.log(2)) #(1/base_prob) ** (1/len(encoded_sentence))\n",
    "    return overall_perplexity, base_each_prob\n",
    "    \n",
    "def find_mask_indexes(model, tokenizer, full_sentence, encoded_sentence, num_mask = None, mask_cutoff = None):\n",
    "    sentence_perplexity, prob_each_index = compute_perplexity(model, tokenizer, full_sentence, encoded_sentence)\n",
    "\n",
    "    indexes_by_prob = [[p,i] for i,p in enumerate(prob_each_index)]\n",
    "    indexes_by_prob = sorted(indexes_by_prob)\n",
    "    \n",
    "    #print(indexes_by_prob)\n",
    "    \n",
    "    if not(num_mask is None):\n",
    "        res = [tu[1] for tu in indexes_by_prob[:num_mask]]\n",
    "    elif not(mask_cutoff is None):\n",
    "        res = []\n",
    "        for p,i in indexes_by_prob:\n",
    "            if p < mask_cutoff:\n",
    "                break\n",
    "            res.append(i)\n",
    "    else:\n",
    "        print(\"ERROR: NEED TYPE OF MASK (EITHER NUMBER OR CUTOFF)\")\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "\n",
    "def compute_loss(model, tokenizer, new_sentence, original_sentence, hyperparameters):\n",
    "    a = hyperparameters['alpha']\n",
    "    b = hyperparameters['beta']\n",
    "    d = hyperparameters['delta']\n",
    "    \n",
    "    new_encoded_sentence = tokenizer(new_sentence, return_tensors = 'pt')['input_ids'][0]\n",
    "    \n",
    "    perplexity, _ = compute_perplexity(model, tokenizer, new_sentence, new_encoded_sentence)\n",
    "    coherence = sentence_coherence_score_single(new_sentence)\n",
    "    similarity = similarity_score_single(new_sentence, original_sentence)\n",
    "    \n",
    "    objective = a * perplexity + b * coherence + d * similarity\n",
    "    loss = -objective\n",
    "    \n",
    "    return loss, perplexity, coherence, similarity\n",
    "\n",
    "def fill_masked_indexes(ft_model, ft_tokenizer, sentence, encoded_sentence, mask_indexes):\n",
    "    def get_inference(all_ids, idx):\n",
    "        isGPT = MODEL_NAME[:4] == 'gpt2'\n",
    "        with torch.inference_mode():  \n",
    "            if len(all_ids.size()) == 1 and not(isGPT):\n",
    "                all_ids = torch.unsqueeze(all_ids,0)\n",
    "            end_model = ft_model(input_ids = all_ids)\n",
    "            logits = end_model.logits\n",
    "            if isGPT: res = torch.argmax(logits[idx])\n",
    "            else: res = torch.argmax(logits[0][idx])\n",
    "            print(res)\n",
    "            return res\n",
    "    \n",
    "    curr_encoded_sentence = torch.clone(encoded_sentence)\n",
    "    for idx in mask_indexes:\n",
    "        new_token = get_inference(curr_encoded_sentence, idx)\n",
    "        curr_encoded_sentence[idx] = new_token\n",
    "    \n",
    "    return curr_encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7fe8c6b-bdf4-4a4f-803c-b99816b91ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1996, 8044, 1999, 1996, 3712, 2024, 2200, 2630, 2651, 1012,  102])\n",
      "[11, 0, 2]\n",
      "tensor(1012)\n",
      "tensor(1012)\n",
      "tensor(8044)\n",
      "tensor([1012, 1996, 8044, 1999, 1996, 3712, 2024, 2200, 2630, 2651, 1012, 1012])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The clouds in the sky are very blue today.\"\n",
    "encoded_sentence = BASE_TOKENIZER(sentence, return_tensors = 'pt')['input_ids'][0]\n",
    "print(encoded_sentence)\n",
    "#print(BASE_TOKENIZER(sentence, return_tensors = 'pt')['input_ids'])\n",
    "mask_indexes = find_mask_indexes(BASE_MODEL, BASE_TOKENIZER, sentence, encoded_sentence, num_mask = 3)\n",
    "print(mask_indexes)\n",
    "final_sentence_encoded = fill_masked_indexes(FT_MODEL, FT_TOKENIZER, sentence, encoded_sentence, mask_indexes)\n",
    "print(final_sentence_encoded)\n",
    "#print(FT_TOKENIZER.decode(final_sentence_encoded)) # COMPUTE CANNOT HANDLE THIS COMPUTATION -> FIX ON GPU/CREDITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7ca76-d010-4e31-b22d-38f4eedd2c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1831e8bb-e7da-4518-84a2-893dc0deb7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost:  <_io.TextIOWrapper name='data/.DS_Store' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# assign directory\n",
    "directory = 'data'\n",
    "\n",
    "all_sents = []\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        f1 = open(f, 'r')\n",
    "        #print(f1.readlines())\n",
    "        try:\n",
    "            lines = \"\".join(f1.readlines())\n",
    "            sents = compute_sentences_single_essay(lines)\n",
    "            all_sents += sents\n",
    "        except:\n",
    "            print(\"Lost: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de87e268-69ed-49e8-b39c-c44ee4a0f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(all_sents)\n",
    "\n",
    "train, dev, test = 0.8, 0.1, 0.1\n",
    "train_amt, dev_amt = int(len(all_sents) * 0.8), int(len(all_sents) * 0.1)\n",
    "test_amt = len(all_sents) - train_amt - dev_amt\n",
    "\n",
    "train_sents = all_sents[:train_amt]\n",
    "dev_sents = all_sents[train_amt:train_amt + dev_amt]\n",
    "test_sents = all_sents[train_amt+dev_amt:]\n",
    "\n",
    "text_file = open(\"train_sents.txt\", \"w\")\n",
    "text_file.write(\"\\n\".join(train_sents))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"dev_sents.txt\", \"w\")\n",
    "text_file.write(\"\\n\".join(dev_sents))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"test_sents.txt\", \"w\")\n",
    "text_file.write(\"\\n\".join(test_sents))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e22fa2-6a19-4640-8972-6ca4406c0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train_loss_vals.txt\", np.zeros(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ad87cc-61b5-4549-9a3b-b568186bcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"completed_sents.txt\", \"w\")\n",
    "text_file.write(\"0\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce23ec1-5c8c-470c-97cf-d315d3e1b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other symptoms may include nausea, shortness of breath, sweating, fatigue, and lightheadedness.\n",
      "(tensor(-7.0022), tensor(22.2508), 1.0, tensor(1.0000))\n",
      "Symptoms of altitude sickness include nausea, headaches, fatigue, and shortness of breath.\n",
      "(tensor(-7.0022), tensor(21.8409), 1.0, tensor(1.0000))\n",
      "Blessings can also come in the form of opportunities.\n",
      "(tensor(-6.2055), tensor(54.5604), 0.8, tensor(1.))\n",
      "Anatomy is a very important field of study, as it helps us to understand how the body works and how it functions.\n",
      "(tensor(-6.7222), tensor(21.9384), 0.93, tensor(1.0000))\n",
      "By forming an alliance, states can pool their resources and share risks.\n",
      "(tensor(-6.6025), tensor(24.9393), 0.9, tensor(1.0000))\n",
      "John Grisham is another popular author, and his novels have sold over 300 million copies.\n",
      "(tensor(-6.6012), tensor(11.5450), 0.9, tensor(1.0000))\n",
      "They are produced by the B cells of the immune system, and they recognize and bind to specific antigens, initiating a cascade of events that leads to the destruction of the foreign substance.\n",
      "(tensor(-6.7206), tensor(6.4039), 0.93, tensor(1.))\n",
      "This force is caused by the attraction between the positively charged protons in the nucleus and the negatively charged electrons in the shells.\n",
      "(tensor(-7.0011), tensor(10.8809), 1.0, tensor(1.0000))\n",
      "The biosphere is important for many reasons.\n",
      "(tensor(-6.2102), tensor(101.6793), 0.8, tensor(1.))\n",
      "Blackness is a source of both strength and struggle.\n",
      "(tensor(-6.6052), tensor(51.8036), 0.9, tensor(1.0000))\n"
     ]
    }
   ],
   "source": [
    "amt_keys = np.loadtxt(KEYS_PATH)\n",
    "hyperparameters = {'alpha': 0.0001, 'beta': 4, 'delta': 3}\n",
    "for i in range(10):\n",
    "    file = random.randint(0,len(amt_keys)-1)\n",
    "    sent = random.randint(0,amt_keys[file]-1)\n",
    "    \n",
    "    all_nouns = []\n",
    "    file1 = open('nounlist.txt', 'r')\n",
    "    lines = file1.readlines()\n",
    "    all_nouns = [i.strip() for i in lines]\n",
    "    \n",
    "    noun = all_nouns[file//3]\n",
    "    vers = file%3\n",
    "    \n",
    "    filepath = FOLDER_PATH + str(noun) + str(vers)\n",
    "    \n",
    "    f1 = open(filepath, 'r')\n",
    "    lines = \"\".join(f1.readlines())\n",
    "    \n",
    "    #print(lines)\n",
    "    \n",
    "    sents = compute_sentences_single_essay(lines)\n",
    "    sentence = sents[sent]\n",
    "    print(sentence)\n",
    "    print(compute_loss(BASE_MODEL, BASE_TOKENIZER, sentence, sentence, hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e043be-f86d-40aa-b7c0-2e9e4d4bdcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
