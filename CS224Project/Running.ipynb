{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e8910d-b302-41a6-ae8d-2b36cdcc0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import random\n",
    "import bisect\n",
    "from bisect import bisect_left, bisect_right\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b24a14-d998-4087-b6b2-0f97cbc91d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'average_generated_prob': 0,\n",
       "   'completely_generated_prob': 0.11111111111111108,\n",
       "   'overall_burstiness': 0,\n",
       "   'paragraphs': [{'completely_generated_prob': 0.11111111111111108,\n",
       "     'num_sentences': 1,\n",
       "     'start_sentence_index': 0}],\n",
       "   'sentences': [{'generated_prob': 0,\n",
       "     'perplexity': 92,\n",
       "     'sentence': 'I am running to the gym.'}]}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTZERO_API_URL = \"https://api.gptzero.me/v2/predict/text\"\n",
    "todo = {\"document\": \"I am running to the gym.\"}\n",
    "response = requests.post(GPTZERO_API_URL, json=todo)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3824db4-f904-4b78-a930-988c926d3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"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\"\n",
    "openai.api_key = API_KEY\n",
    "MODEL_ENGINE = \"text-davinci-003\"\n",
    "FOLDER_PATH = \"data/\"\n",
    "KEYS_PATH = \"keys.txt\"\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "Cls = transformers.AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = Cls.from_pretrained(MODEL_NAME)\n",
    "if isinstance(BASE_MODEL, transformers.GPT2LMHeadModel):\n",
    "    BASE_MODEL.transformer.gradient_checkpointing_enable()\n",
    "BASE_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if BASE_TOKENIZER.pad_token_id is None:\n",
    "    if Cls == transformers.AutoModelForCausalLM:\n",
    "        BASE_TOKENIZER.pad_token = BASE_TOKENIZER.eos_token\n",
    "    else:\n",
    "        print(\"Adding pad token to tokenizer\")\n",
    "        BASE_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        BASE_TOKENIZER.pad_token = '[PAD]'\n",
    "        \n",
    "FT_MODEL = Cls.from_pretrained(MODEL_NAME)\n",
    "if isinstance(FT_MODEL, transformers.GPT2LMHeadModel):\n",
    "    FT_MODEL.transformer.gradient_checkpointing_enable()\n",
    "FT_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if FT_TOKENIZER.pad_token_id is None:\n",
    "    if Cls == transformers.AutoModelForCausalLM:\n",
    "        FT_TOKENIZER.pad_token = FT_TOKENIZER.eos_token\n",
    "    else:\n",
    "        print(\"Adding pad token to tokenizer\")\n",
    "        FT_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        FT_TOKENIZER.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c90783d-78cc-4ac0-aa43-2240d03f3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_coherence(sentence):\n",
    "    modified_prompt = \"\"\"answer in one word yes or no: does this make sense as a sentence \\\"\"\"\" + sentence + \"\"\"\\\"\"\"\"\n",
    "    print(modified_prompt)\n",
    "    # Generate a response\n",
    "    completion = openai.Completion.create(\n",
    "        engine=MODEL_ENGINE,\n",
    "        prompt=modified_prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].text.strip()\n",
    "    print(res)\n",
    "    if res.lower()[:2] == \"no\":\n",
    "        return \"Incoherent\"\n",
    "    elif res.lower()[:3] == \"yes\":\n",
    "        return \"Coherent\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def sentence_embedding(input_sentence, return_type = \"torch\"):\n",
    "    response = openai.Embedding.create(\n",
    "    input=input_sentence,\n",
    "    engine=\"text-similarity-davinci-001\")\n",
    "    res = response.data[0]['embedding']\n",
    "    \n",
    "    if return_type.lower() == \"np\" or return_type.lower() == \"numpy\":\n",
    "        return np.array(res)\n",
    "    elif return_type.lower() == \"list\":\n",
    "        return res\n",
    "    else:\n",
    "        return torch.tensor(res)\n",
    "    \n",
    "def similarity_score_single(sentence1, sentence2):\n",
    "    embed1 = sentence_embedding(sentence1, \"torch\")\n",
    "    embed2 = sentence_embedding(sentence2, \"torch\")\n",
    "    norm1 = torch.sqrt(torch.sum(embed1 * embed1))\n",
    "    norm2 = torch.sqrt(torch.sum(embed2 * embed2))\n",
    "    numerator = torch.dot(embed1, embed2)\n",
    "    denominator = norm1 * norm2\n",
    "    return numerator/denominator\n",
    "\n",
    "def sentence_coherence_score_single(input_sentence):\n",
    "    modified_prompt = \"Evaluate the coherence score of this sentence as a value between 0 and 1:\\n\\n\" + input_sentence\n",
    "    response = openai.Completion.create(\n",
    "      model=MODEL_ENGINE,\n",
    "      prompt=modified_prompt,\n",
    "      temperature=0,\n",
    "      max_tokens=60,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0\n",
    "    )\n",
    "    res = response.choices[0]['text'].strip()\n",
    "    return float(res)\n",
    "\n",
    "def compute_sentences(responses):\n",
    "    essays = [i.split(\"\\n\") for i in responses]\n",
    "    sentences = []\n",
    "    for essay_li in essays:\n",
    "        essay_sents = []\n",
    "        for portion in essay_li:\n",
    "            if len(portion.strip()) == 0:\n",
    "                continue\n",
    "            add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "            essay_sents += add_li\n",
    "            #print(essay_sents)\n",
    "        sentences.append(essay_sents)\n",
    "    return sentences\n",
    "    \n",
    "def compute_sentences_single_essay(essay):\n",
    "    essay_li = essay.split(\"\\n\")\n",
    "    essay_sents = []\n",
    "    for portion in essay_li:\n",
    "        if len(portion.strip()) == 0:\n",
    "            continue\n",
    "        add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "        essay_sents += add_li\n",
    "    return essay_sents\n",
    "\n",
    "def collect_data(word):\n",
    "    num = 3\n",
    "    prompt = \"Write a long essay about \" + word\n",
    "    completion = openai.Completion.create(\n",
    "        engine=MODEL_ENGINE,\n",
    "        prompt=prompt,\n",
    "        max_tokens=3500,\n",
    "        n=num,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "        \n",
    "    responses = [completion.choices[i].text for i in range(len(completion.choices))]\n",
    "    \n",
    "    sentences_per = compute_sentences(responses)\n",
    "    lens = [len(sen) for sen in sentences_per]\n",
    "    \n",
    "    for i,essay in enumerate(responses):\n",
    "        filepath = FOLDER_PATH + word + str(i)\n",
    "        f = open(filepath, \"w\")\n",
    "        f.write(essay)\n",
    "        f.close()\n",
    "        \n",
    "    return lens\n",
    "        \n",
    "def gen_data(num_words):\n",
    "    all_nouns = []\n",
    "    file1 = open('nounlist.txt', 'r')\n",
    "    lines = file1.readlines()\n",
    "    all_nouns = [i.strip() for i in lines]\n",
    "    \n",
    "    amt_keys = np.loadtxt(KEYS_PATH)\n",
    "    prev_gen = np.sum(amt_keys)\n",
    "        \n",
    "    start_index = int(len(amt_keys)/3)\n",
    "    stop_index = min(start_index + num_words, len(all_nouns))\n",
    "    for i in range(start_index,stop_index):\n",
    "        print(i)\n",
    "        lens = collect_data(all_nouns[i])\n",
    "        \n",
    "        amt_keys = np.append(amt_keys, np.array(lens))\n",
    "        total_gen = np.sum(amt_keys)\n",
    "        \n",
    "        print(\"Generated: \" + str(lens) + \" for: \" + str(all_nouns[i]))\n",
    "        print(\"Total generated now: \" + str(total_gen) + \", Generated this iteration: \" + str(total_gen - prev_gen))\n",
    "        \n",
    "        np.savetxt(KEYS_PATH, amt_keys)\n",
    "        \n",
    "        time.sleep(60)\n",
    "        \n",
    "def get_prob(model, tokenizer, full_sentence, encoded_sentence):    \n",
    "    isGPT = MODEL_NAME[:4] == 'gpt2'\n",
    "    def get_word_prob(ids_so_far, true_token):\n",
    "        with torch.inference_mode():\n",
    "            if len(ids_so_far.size()) == 1 and not(isGPT):\n",
    "                ids_so_far = torch.unsqueeze(ids_so_far,0)\n",
    "            end_model = model(input_ids = ids_so_far)\n",
    "            logits = end_model.logits\n",
    "            #print(ids_so_far)\n",
    "            #print(tokenizer.decode(ids_so_far))\n",
    "            #print(logits.size())\n",
    "            all_probs = torch.nn.functional.softmax(logits, dim = -1)\n",
    "            \n",
    "            if isGPT: probability = all_probs[-1][true_token]\n",
    "            else: probability =  all_probs[0][-1][true_token]\n",
    "                \n",
    "            return probability\n",
    "    \n",
    "    all_probs = torch.zeros(len(encoded_sentence))\n",
    "\n",
    "    total_log_prob = 0\n",
    "    #print(all_probs)\n",
    "    for i in range(0,len(encoded_sentence)):\n",
    "        word_cond_prob = get_word_prob(encoded_sentence[:i+1], encoded_sentence[i])\n",
    "        all_probs[i] = word_cond_prob\n",
    "        total_log_prob += np.log(word_cond_prob)\n",
    "    \n",
    "    return total_log_prob, all_probs\n",
    "\n",
    "def compute_perplexity(model, tokenizer, full_sentence, encoded_sentence):\n",
    "    base_log_prob, base_each_prob = get_prob(model, tokenizer, full_sentence, encoded_sentence)\n",
    "    #print(base_log_prob)\n",
    "    N = len(encoded_sentence)\n",
    "    \n",
    "    overall_perplexity = 2 ** (-(1/N) * base_log_prob/np.log(2)) #(1/base_prob) ** (1/len(encoded_sentence))\n",
    "    return overall_perplexity, base_each_prob\n",
    "    \n",
    "def find_mask_indexes(model, tokenizer, full_sentence, encoded_sentence, num_mask = None, mask_cutoff = None):\n",
    "    sentence_perplexity, prob_each_index = compute_perplexity(model, tokenizer, full_sentence, encoded_sentence)\n",
    "\n",
    "    indexes_by_prob = [[p,i] for i,p in enumerate(prob_each_index)]\n",
    "    indexes_by_prob = sorted(indexes_by_prob)\n",
    "    \n",
    "    #print(indexes_by_prob)\n",
    "    \n",
    "    if not(num_mask is None):\n",
    "        res = [tu[1] for tu in indexes_by_prob[:num_mask]]\n",
    "    elif not(mask_cutoff is None):\n",
    "        res = []\n",
    "        for p,i in indexes_by_prob:\n",
    "            if p < mask_cutoff:\n",
    "                break\n",
    "            res.append(i)\n",
    "    else:\n",
    "        print(\"ERROR: NEED TYPE OF MASK (EITHER NUMBER OR CUTOFF)\")\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "\n",
    "def compute_loss(model, tokenizer, new_sentence, original_sentence, hyperparameters):\n",
    "    a = hyperparameters['alpha']\n",
    "    b = hyperparameters['beta']\n",
    "    d = hyperparameters['delta']\n",
    "    \n",
    "    new_encoded_sentence = tokenizer(new_sentence, return_tensors = 'pt')['input_ids'][0]\n",
    "    \n",
    "    perplexity, _ = compute_perplexity(model, tokenizer, new_sentence, new_encoded_sentence)\n",
    "    coherence = sentence_coherence_score_single(new_sentence)\n",
    "    similarity = similarity_score_single(new_sentence, original_sentence)\n",
    "    \n",
    "    objective = a * perplexity + b * coherence + d * similarity\n",
    "    loss = -objective\n",
    "    \n",
    "    return loss, perplexity, coherence, similarity\n",
    "\n",
    "def fill_masked_indexes(ft_model, ft_tokenizer, sentence, encoded_sentence, mask_indexes):\n",
    "    def get_inference(all_ids, idx):\n",
    "        isGPT = MODEL_NAME[:4] == 'gpt2'\n",
    "        with torch.inference_mode():  \n",
    "            if len(all_ids.size()) == 1 and not(isGPT):\n",
    "                all_ids = torch.unsqueeze(all_ids,0)\n",
    "            end_model = ft_model(input_ids = all_ids)\n",
    "            logits = end_model.logits\n",
    "            if isGPT: res = torch.argmax(logits[idx])\n",
    "            else: res = torch.argmax(logits[0][idx])\n",
    "            print(res)\n",
    "            return res\n",
    "    \n",
    "    curr_encoded_sentence = torch.clone(encoded_sentence)\n",
    "    for idx in mask_indexes:\n",
    "        new_token = get_inference(curr_encoded_sentence, idx)\n",
    "        curr_encoded_sentence[idx] = new_token\n",
    "    \n",
    "    return curr_encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fe8c6b-bdf4-4a4f-803c-b99816b91ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1996, 8044, 1999, 1996, 3712, 2024, 2200, 2630, 2651, 1012,  102])\n",
      "[11, 0, 2]\n",
      "tensor(1012)\n",
      "tensor(1012)\n",
      "tensor(8044)\n",
      "tensor([1012, 1996, 8044, 1999, 1996, 3712, 2024, 2200, 2630, 2651, 1012, 1012])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The clouds in the sky are very blue today.\"\n",
    "encoded_sentence = BASE_TOKENIZER(sentence, return_tensors = 'pt')['input_ids'][0]\n",
    "print(encoded_sentence)\n",
    "#print(BASE_TOKENIZER(sentence, return_tensors = 'pt')['input_ids'])\n",
    "mask_indexes = find_mask_indexes(BASE_MODEL, BASE_TOKENIZER, sentence, encoded_sentence, num_mask = 3)\n",
    "print(mask_indexes)\n",
    "final_sentence_encoded = fill_masked_indexes(FT_MODEL, FT_TOKENIZER, sentence, encoded_sentence, mask_indexes)\n",
    "print(final_sentence_encoded)\n",
    "#print(FT_TOKENIZER.decode(final_sentence_encoded)) # COMPUTE CANNOT HANDLE THIS COMPUTATION -> FIX ON GPU/CREDITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce23ec1-5c8c-470c-97cf-d315d3e1b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concept of abundance is closely linked to the idea of abundance thinking.\n",
      "(tensor(-6.6027), tensor(27.2214), 0.9, tensor(1.))\n",
      "Accuracy is also important in communication.\n",
      "(tensor(-6.0294), tensor(293.9554), 0.75, tensor(1.0000))\n",
      "Additionally, an affair can lead to the end of a marriage or long-term relationship.\n",
      "(tensor(-6.2011), tensor(11.3739), 0.8, tensor(1.))\n",
      "Administrators must also be able to work with a variety of different people.\n",
      "(tensor(-6.0053), tensor(52.9603), 0.75, tensor(1.))\n",
      "It is an art form that requires a great deal of skill, dedication, and practice in order to master.\n",
      "(tensor(-6.7211), tensor(10.7678), 0.93, tensor(1.))\n",
      "The diagnosis and treatment of abnormal behavior is typically a complex process.\n",
      "(tensor(-6.4443), tensor(42.6803), 0.86, tensor(1.))\n",
      "Admission is a process that can be a difficult one for both the student and the institution.\n",
      "(tensor(-6.4415), tensor(14.8237), 0.86, tensor(1.))\n",
      "It is important to consider cost, location, size, and amenities when choosing accommodation.\n",
      "(tensor(-7.0017), tensor(16.8403), 1.0, tensor(1.))\n",
      "Positive affect can help us to feel more connected to others, be more creative, and have a more positive outlook on life.\n",
      "(tensor(-6.6009), tensor(8.9199), 0.9, tensor(1.))\n",
      "It is best known for its suite of creative and digital media products, including Photoshop, Illustrator, Acrobat, and InDesign.\n",
      "(tensor(-6.6009), tensor(8.7983), 0.9, tensor(1.))\n"
     ]
    }
   ],
   "source": [
    "amt_keys = np.loadtxt(KEYS_PATH)\n",
    "hyperparameters = {'alpha': 0.0001, 'beta': 4, 'delta': 3}\n",
    "for i in range(10):\n",
    "    file = random.randint(0,len(amt_keys)-1)\n",
    "    sent = random.randint(0,amt_keys[file]-1)\n",
    "    \n",
    "    all_nouns = []\n",
    "    file1 = open('nounlist.txt', 'r')\n",
    "    lines = file1.readlines()\n",
    "    all_nouns = [i.strip() for i in lines]\n",
    "    \n",
    "    noun = all_nouns[file//3]\n",
    "    vers = file%3\n",
    "    \n",
    "    filepath = FOLDER_PATH + str(noun) + str(vers)\n",
    "    \n",
    "    f1 = open(filepath, 'r')\n",
    "    lines = \"\".join(f1.readlines())\n",
    "    \n",
    "    #print(lines)\n",
    "    \n",
    "    sents = compute_sentences_single_essay(lines)\n",
    "    sentence = sents[sent]\n",
    "    print(sentence)\n",
    "    print(compute_loss(BASE_MODEL, BASE_TOKENIZER, sentence, sentence, hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e043be-f86d-40aa-b7c0-2e9e4d4bdcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
