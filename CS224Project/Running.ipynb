{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e8910d-b302-41a6-ae8d-2b36cdcc0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import random\n",
    "import bisect\n",
    "from bisect import bisect_left, bisect_right\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b24a14-d998-4087-b6b2-0f97cbc91d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'average_generated_prob': 0,\n",
       "   'completely_generated_prob': 0.11111111111111108,\n",
       "   'overall_burstiness': 0,\n",
       "   'paragraphs': [{'completely_generated_prob': 0.11111111111111108,\n",
       "     'num_sentences': 1,\n",
       "     'start_sentence_index': 0}],\n",
       "   'sentences': [{'generated_prob': 0,\n",
       "     'perplexity': 92,\n",
       "     'sentence': 'I am running to the gym.'}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTZERO_API_URL = \"https://api.gptzero.me/v2/predict/text\"\n",
    "todo = {\"document\": \"I am running to the gym.\"}\n",
    "response = requests.post(GPTZERO_API_URL, json=todo)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3824db4-f904-4b78-a930-988c926d3309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac052194f4544f1a19aa1c48e7f9591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561c3f33d71841ad82803825a0f03c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ecbd3efd6d4b7cbab90a0ad2e77d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"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\"\n",
    "openai.api_key = API_KEY\n",
    "MODEL_ENGINE = \"text-davinci-003\"\n",
    "FOLDER_PATH = \"data/\"\n",
    "KEYS_PATH = \"keys.txt\"\n",
    "\n",
    "MODEL_NAME = 'prajjwal1/bert-medium'\n",
    "Cls = transformers.AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = Cls.from_pretrained(MODEL_NAME)\n",
    "if isinstance(BASE_MODEL, transformers.GPT2LMHeadModel):\n",
    "    BASE_MODEL.transformer.gradient_checkpointing_enable()\n",
    "BASE_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if BASE_TOKENIZER.pad_token_id is None:\n",
    "    if Cls == transformers.AutoModelForCausalLM:\n",
    "        BASE_TOKENIZER.pad_token = BASE_TOKENIZER.eos_token\n",
    "    else:\n",
    "        print(\"Adding pad token to tokenizer\")\n",
    "        BASE_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        BASE_TOKENIZER.pad_token = '[PAD]'\n",
    "        \n",
    "FT_MODEL = Cls.from_pretrained(MODEL_NAME)\n",
    "if isinstance(FT_MODEL, transformers.GPT2LMHeadModel):\n",
    "    FT_MODEL.transformer.gradient_checkpointing_enable()\n",
    "FT_TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if FT_TOKENIZER.pad_token_id is None:\n",
    "    if Cls == transformers.AutoModelForCausalLM:\n",
    "        FT_TOKENIZER.pad_token = FT_TOKENIZER.eos_token\n",
    "    else:\n",
    "        print(\"Adding pad token to tokenizer\")\n",
    "        FT_TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        FT_TOKENIZER.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c90783d-78cc-4ac0-aa43-2240d03f3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_coherence(sentence):\n",
    "    modified_prompt = \"\"\"answer in one word yes or no: does this make sense as a sentence \\\"\"\"\" + sentence + \"\"\"\\\"\"\"\"\n",
    "    print(modified_prompt)\n",
    "    # Generate a response\n",
    "    completion = openai.Completion.create(\n",
    "        engine=MODEL_ENGINE,\n",
    "        prompt=modified_prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].text.strip()\n",
    "    print(res)\n",
    "    if res.lower()[:2] == \"no\":\n",
    "        return \"Incoherent\"\n",
    "    elif res.lower()[:3] == \"yes\":\n",
    "        return \"Coherent\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def sentence_embedding(input_sentence, return_type = \"torch\"):\n",
    "    response = openai.Embedding.create(\n",
    "    input=input_sentence,\n",
    "    engine=\"text-similarity-davinci-001\")\n",
    "    res = response.data[0]['embedding']\n",
    "    \n",
    "    if return_type.lower() == \"np\" or return_type.lower() == \"numpy\":\n",
    "        return np.array(res)\n",
    "    elif return_type.lower() == \"list\":\n",
    "        return res\n",
    "    else:\n",
    "        return torch.tensor(res)\n",
    "    \n",
    "def similarity_score_single(sentence1, sentence2):\n",
    "    embed1 = sentence_embedding(sentence1, \"torch\")\n",
    "    embed2 = sentence_embedding(sentence2, \"torch\")\n",
    "    norm1 = torch.sqrt(torch.sum(embed1 * embed1))\n",
    "    norm2 = torch.sqrt(torch.sum(embed2 * embed2))\n",
    "    numerator = torch.dot(embed1, embed2)\n",
    "    denominator = norm1 * norm2\n",
    "    return numerator/denominator\n",
    "\n",
    "def sentence_coherence_score_single(input_sentence):\n",
    "    modified_prompt = \"Evaluate the coherence score of this sentence as a value between 0 and 1:\\n\\n\" + input_sentence\n",
    "    response = openai.Completion.create(\n",
    "      model=MODEL_ENGINE,\n",
    "      prompt=modified_prompt,\n",
    "      temperature=0,\n",
    "      max_tokens=60,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0\n",
    "    )\n",
    "    res = response.choices[0]['text'].strip()\n",
    "    return float(res)\n",
    "\n",
    "def compute_sentences(responses):\n",
    "    essays = [i.split(\"\\n\") for i in responses]\n",
    "    sentences = []\n",
    "    for essay_li in essays:\n",
    "        essay_sents = []\n",
    "        for portion in essay_li:\n",
    "            if len(portion.strip()) == 0:\n",
    "                continue\n",
    "            add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "            essay_sents += add_li\n",
    "            #print(essay_sents)\n",
    "        sentences.append(essay_sents)\n",
    "    return sentences\n",
    "    \n",
    "def compute_sentences_single_essay(essay):\n",
    "    essay_li = essay.split(\"\\n\")\n",
    "    essay_sents = []\n",
    "    for portion in essay_li:\n",
    "        if len(portion.strip()) == 0:\n",
    "            continue\n",
    "        add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "        essay_sents += add_li\n",
    "    return essay_sents\n",
    "\n",
    "def collect_data(word):\n",
    "    num = 3\n",
    "    prompt = \"Write a long essay about \" + word\n",
    "    completion = openai.Completion.create(\n",
    "        engine=MODEL_ENGINE,\n",
    "        prompt=prompt,\n",
    "        max_tokens=3500,\n",
    "        n=num,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "        \n",
    "    responses = [completion.choices[i].text for i in range(len(completion.choices))]\n",
    "    \n",
    "    sentences_per = compute_sentences(responses)\n",
    "    lens = [len(sen) for sen in sentences_per]\n",
    "    \n",
    "    for i,essay in enumerate(responses):\n",
    "        filepath = FOLDER_PATH + word + str(i)\n",
    "        f = open(filepath, \"w\")\n",
    "        f.write(essay)\n",
    "        f.close()\n",
    "        \n",
    "    return lens\n",
    "        \n",
    "def gen_data(num_words):\n",
    "    all_nouns = []\n",
    "    file1 = open('nounlist.txt', 'r')\n",
    "    lines = file1.readlines()\n",
    "    all_nouns = [i.strip() for i in lines]\n",
    "    \n",
    "    amt_keys = np.loadtxt(KEYS_PATH)\n",
    "    prev_gen = np.sum(amt_keys)\n",
    "        \n",
    "    start_index = int(len(amt_keys)/3)\n",
    "    stop_index = min(start_index + num_words, len(all_nouns))\n",
    "    for i in range(start_index,stop_index):\n",
    "        print(i)\n",
    "        lens = collect_data(all_nouns[i])\n",
    "        \n",
    "        amt_keys = np.append(amt_keys, np.array(lens))\n",
    "        total_gen = np.sum(amt_keys)\n",
    "        \n",
    "        print(\"Generated: \" + str(lens) + \" for: \" + str(all_nouns[i]))\n",
    "        print(\"Total generated now: \" + str(total_gen) + \", Generated this iteration: \" + str(total_gen - prev_gen))\n",
    "        \n",
    "        np.savetxt(KEYS_PATH, amt_keys)\n",
    "        \n",
    "        time.sleep(60)\n",
    "        \n",
    "def get_prob(model, tokenizer, full_sentence, encoded_sentence):    \n",
    "    \n",
    "    def get_word_prob(ids_so_far, true_token):\n",
    "        with torch.inference_mode():  \n",
    "            end_model = model(input_ids = ids_so_far)\n",
    "            logits = end_model.logits\n",
    "            #print(ids_so_far)\n",
    "            #print(tokenizer.decode(ids_so_far))\n",
    "            #print(logits.size())\n",
    "            all_probs = torch.nn.functional.softmax(logits, dim = -1)\n",
    "            return all_probs[-1][true_token]\n",
    "    \n",
    "    all_probs = torch.zeros(len(encoded_sentence))\n",
    "\n",
    "    total_log_prob = 0\n",
    "    #print(all_probs)\n",
    "    for i in range(0,len(encoded_sentence)):\n",
    "        word_cond_prob = get_word_prob(encoded_sentence[:i+1], encoded_sentence[i])\n",
    "        all_probs[i] = word_cond_prob\n",
    "        total_log_prob += np.log(word_cond_prob)\n",
    "    \n",
    "    return total_log_prob, all_probs\n",
    "\n",
    "def compute_perplexity(model, tokenizer, full_sentence, encoded_sentence):\n",
    "    base_log_prob, base_each_prob = get_prob(model, tokenizer, full_sentence, encoded_sentence)\n",
    "    #print(base_log_prob)\n",
    "    N = len(encoded_sentence)\n",
    "    \n",
    "    overall_perplexity = 2 ** (-(1/N) * base_log_prob/np.log(2)) #(1/base_prob) ** (1/len(encoded_sentence))\n",
    "    return overall_perplexity, base_each_prob\n",
    "    \n",
    "def find_mask_indexes(model, tokenizer, full_sentence, encoded_sentence, num_mask = None, mask_cutoff = None):\n",
    "    sentence_perplexity, prob_each_index = compute_perplexity(model, tokenizer, full_sentence, encoded_sentence)\n",
    "\n",
    "    indexes_by_prob = [[p,i] for i,p in enumerate(prob_each_index)]\n",
    "    indexes_by_prob = sorted(indexes_by_prob)\n",
    "    \n",
    "    #print(indexes_by_prob)\n",
    "    \n",
    "    if not(num_mask is None):\n",
    "        res = [tu[1] for tu in indexes_by_prob[:num_mask]]\n",
    "    elif not(mask_cutoff is None):\n",
    "        res = []\n",
    "        for p,i in indexes_by_prob:\n",
    "            if p < mask_cutoff:\n",
    "                break\n",
    "            res.append(i)\n",
    "    else:\n",
    "        print(\"ERROR: NEED TYPE OF MASK (EITHER NUMBER OR CUTOFF)\")\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "\n",
    "def compute_loss(model, tokenizer, new_sentence, original_sentence, hyperparameters):\n",
    "    a = hyperparameters['alpha']\n",
    "    b = hyperparameters['beta']\n",
    "    d = hyperparameters['delta']\n",
    "    \n",
    "    new_encoded_sentence = tokenizer(new_sentence, return_tensors = 'pt')['input_ids'][0]\n",
    "    \n",
    "    perplexity, _ = compute_perplexity(model, tokenizer, new_sentence, new_encoded_sentence)\n",
    "    coherence = sentence_coherence_score_single(new_sentence)\n",
    "    similarity = similarity_score_single(new_sentence, original_sentence)\n",
    "    \n",
    "    objective = a * perplexity + b * coherence + d * similarity\n",
    "    loss = -objective\n",
    "    \n",
    "    return loss, perplexity, coherence, similarity\n",
    "\n",
    "def fill_masked_indexes(ft_model, ft_tokenizer, sentence, encoded_sentence, mask_indexes):\n",
    "    def get_inference(all_ids, idx):\n",
    "        with torch.inference_mode():  \n",
    "            end_model = ft_model(input_ids = all_ids)\n",
    "            logits = end_model.logits\n",
    "            res = torch.argmax(logits[idx])\n",
    "            print(res)\n",
    "            return res\n",
    "    \n",
    "    curr_encoded_sentence = torch.clone(encoded_sentence)\n",
    "    for idx in mask_indexes:\n",
    "        new_token = get_inference(curr_encoded_sentence, idx)\n",
    "        curr_encoded_sentence[idx] = new_token\n",
    "    \n",
    "    return curr_encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7fe8c6b-bdf4-4a4f-803c-b99816b91ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 2043, 1045, 2175, 5645, 1010, 1045, 4608, 1037, 2843, 1997, 3869,\n",
      "        1012,  102])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m encoded_sentence \u001b[38;5;241m=\u001b[39m BASE_TOKENIZER(sentence, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_sentence)\n\u001b[0;32m----> 4\u001b[0m mask_indexes \u001b[38;5;241m=\u001b[39m \u001b[43mfind_mask_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_TOKENIZER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask_indexes)\n\u001b[1;32m      6\u001b[0m final_sentence_encoded \u001b[38;5;241m=\u001b[39m fill_masked_indexes(FT_MODEL, FT_TOKENIZER, sentence, encoded_sentence, mask_indexes)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfind_mask_indexes\u001b[0;34m(model, tokenizer, full_sentence, encoded_sentence, num_mask, mask_cutoff)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_mask_indexes\u001b[39m(model, tokenizer, full_sentence, encoded_sentence, num_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, mask_cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 165\u001b[0m     sentence_perplexity, prob_each_index \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     indexes_by_prob \u001b[38;5;241m=\u001b[39m [[p,i] \u001b[38;5;28;01mfor\u001b[39;00m i,p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prob_each_index)]\n\u001b[1;32m    168\u001b[0m     indexes_by_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(indexes_by_prob)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mcompute_perplexity\u001b[0;34m(model, tokenizer, full_sentence, encoded_sentence)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_perplexity\u001b[39m(model, tokenizer, full_sentence, encoded_sentence):\n\u001b[0;32m--> 157\u001b[0m     base_log_prob, base_each_prob \u001b[38;5;241m=\u001b[39m \u001b[43mget_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m#print(base_log_prob)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoded_sentence)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mget_prob\u001b[0;34m(model, tokenizer, full_sentence, encoded_sentence)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m#print(all_probs)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(encoded_sentence)):\n\u001b[0;32m--> 150\u001b[0m     word_cond_prob \u001b[38;5;241m=\u001b[39m \u001b[43mget_word_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_sentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_sentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     all_probs[i] \u001b[38;5;241m=\u001b[39m word_cond_prob\n\u001b[1;32m    152\u001b[0m     total_log_prob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(word_cond_prob)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mget_prob.<locals>.get_word_prob\u001b[0;34m(ids_so_far, true_token)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_word_prob\u001b[39m(ids_so_far, true_token):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():  \n\u001b[0;32m--> 137\u001b[0m         end_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mids_so_far\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m         logits \u001b[38;5;241m=\u001b[39m end_model\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;66;03m#print(ids_so_far)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m#print(tokenizer.decode(ids_so_far))\u001b[39;00m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;66;03m#print(logits.size())\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1478\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1479\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1232\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1230\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1232\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1248\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1249\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1478\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1479\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:972\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 972\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    973\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "sentence = \"When I go fishing, I catch a lot of fish.\"\n",
    "encoded_sentence = BASE_TOKENIZER(sentence, return_tensors = 'pt')['input_ids'][0]\n",
    "print(encoded_sentence)\n",
    "mask_indexes = find_mask_indexes(BASE_MODEL, BASE_TOKENIZER, sentence, encoded_sentence, num_mask = 3)\n",
    "print(mask_indexes)\n",
    "final_sentence_encoded = fill_masked_indexes(FT_MODEL, FT_TOKENIZER, sentence, encoded_sentence, mask_indexes)\n",
    "print(final_sentence_encoded)\n",
    "#print(FT_TOKENIZER.decode(final_sentence_encoded)) # COMPUTE CANNOT HANDLE THIS COMPUTATION -> FIX ON GPU/CREDITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ce23ec1-5c8c-470c-97cf-d315d3e1b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We may feel that we are not able to give our loved one the attention and support they need, or that we are not able to be there for them when they need us.\n",
      "(tensor(-8.3438), tensor(19038.4082), 0.86, tensor(1.))\n",
      "The aardvark is a shy and secretive animal, and it is rarely seen in the wild.\n",
      "(tensor(-11.3194), tensor(47194.0898), 0.9, tensor(1.0000))\n",
      "The album has been praised for its production values and songwriting.\n",
      "(tensor(-9.3231), tensor(28830.9062), 0.86, tensor(1.))\n",
      "Accelerators are used to study the structure of matter and the behavior of particles, and are used to create new materials and technologies.\n",
      "(tensor(-8.4067), tensor(16866.6777), 0.93, tensor(1.))\n",
      "Over the next two centuries, the slave trade grew and enslaved Africans were brought to the United States to work as laborers on plantations and in other industries.\n",
      "(tensor(-8.5936), tensor(19936.1367), 0.9, tensor(1.))\n",
      "Accountants can also pursue higher education and become certified public accountants (CPAs).\n",
      "(tensor(-19.7300), tensor(131299.7188), 0.9, tensor(1.))\n",
      "Achieving success is something that everyone strives for in life.\n",
      "(tensor(-12.1825), tensor(57424.8750), 0.86, tensor(1.0000))\n",
      "In such places, people may not be able to understand each other if they have different accents.\n",
      "(tensor(-8.0479), tensor(20479.1465), 0.75, tensor(1.0000))\n",
      "They believe that it is cruel and unusual punishment and that it does not serve as a deterrent to crime.\n",
      "(tensor(-9.2144), tensor(26143.8301), 0.9, tensor(1.0000))\n",
      "It also means that everyone must be willing to listen to others and take their opinions into consideration when making decisions.\n",
      "(tensor(-10.0926), tensor(33725.7539), 0.93, tensor(1.))\n"
     ]
    }
   ],
   "source": [
    "amt_keys = np.loadtxt(KEYS_PATH)\n",
    "hyperparameters = {'alpha': 0.0001, 'beta': 4, 'delta': 3}\n",
    "for i in range(10):\n",
    "    file = random.randint(0,len(amt_keys)-1)\n",
    "    sent = random.randint(0,amt_keys[file]-1)\n",
    "    \n",
    "    all_nouns = []\n",
    "    file1 = open('nounlist.txt', 'r')\n",
    "    lines = file1.readlines()\n",
    "    all_nouns = [i.strip() for i in lines]\n",
    "    \n",
    "    noun = all_nouns[file//3]\n",
    "    vers = file%3\n",
    "    \n",
    "    filepath = FOLDER_PATH + str(noun) + str(vers)\n",
    "    \n",
    "    f1 = open(filepath, 'r')\n",
    "    lines = \"\".join(f1.readlines())\n",
    "    \n",
    "    #print(lines)\n",
    "    \n",
    "    sents = compute_sentences_single_essay(lines)\n",
    "    sentence = sents[sent]\n",
    "    print(sentence)\n",
    "    print(compute_loss(BASE_MODEL, BASE_TOKENIZER, sentence, sentence, hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1174e3d-966a-4e40-b70c-aaf472ea8ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "score = sentence_coherence_score_single(\"I am going to the gym.\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec7c3f0-7dff-4f99-be10-76eda07133f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9307)\n"
     ]
    }
   ],
   "source": [
    "score = similarity_score_single(\"I am running to the gym.\", \"I am walking to the gym.\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a41c10-a25c-43a8-858a-b4d127b3e3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "Generated: [23, 21, 32] for: acrylic\n",
      "Total generated now: 3036.0, Generated this iteration: 76.0\n",
      "45\n",
      "Generated: [17, 22, 20] for: act\n",
      "Total generated now: 3095.0, Generated this iteration: 135.0\n",
      "46\n",
      "Generated: [21, 25, 27] for: action\n",
      "Total generated now: 3168.0, Generated this iteration: 208.0\n",
      "47\n",
      "Generated: [24, 28, 22] for: activation\n",
      "Total generated now: 3242.0, Generated this iteration: 282.0\n",
      "48\n",
      "Generated: [18, 22, 23] for: activist\n",
      "Total generated now: 3305.0, Generated this iteration: 345.0\n",
      "49\n",
      "Generated: [23, 22, 23] for: activity\n",
      "Total generated now: 3373.0, Generated this iteration: 413.0\n",
      "50\n",
      "Generated: [20, 19, 19] for: actor\n",
      "Total generated now: 3431.0, Generated this iteration: 471.0\n",
      "51\n",
      "Generated: [26, 22, 21] for: actress\n",
      "Total generated now: 3500.0, Generated this iteration: 540.0\n",
      "52\n",
      "Generated: [25, 18, 17] for: acupuncture\n",
      "Total generated now: 3560.0, Generated this iteration: 600.0\n",
      "53\n",
      "Generated: [27, 20, 28] for: ad\n",
      "Total generated now: 3635.0, Generated this iteration: 675.0\n",
      "54\n",
      "Generated: [18, 18, 23] for: adaptation\n",
      "Total generated now: 3694.0, Generated this iteration: 734.0\n",
      "55\n",
      "Generated: [29, 20, 21] for: adapter\n",
      "Total generated now: 3764.0, Generated this iteration: 804.0\n"
     ]
    }
   ],
   "source": [
    "gen_data(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9da24ff0-eb04-4541-a681-ac3b8b3dc634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a long essay about cars\n",
      "['\\n\\nCars are one of the most important inventions of the modern age. They have become an integral part of our lives, providing us with a convenient and comfortable way to travel. Cars have revolutionized the way we travel, allowing us to go farther and faster than ever before.\\n\\nCars are powered by an internal combustion engine, which converts fuel into energy. This energy is used to turn the wheels of the car, allowing it to move. Cars come in a variety of sizes, shapes, and types, from small compact cars to large luxury vehicles. They can be powered by gasoline, diesel, or electricity.\\n\\nCars are incredibly complex machines, with many different components and systems that work together to make them run. The engine, transmission, brakes, suspension, and steering are all important components of a car. Each of these systems must be in good condition for the car to run properly.\\n\\nCars also come with a variety of safety features, such as airbags and seat belts. These safety features can help protect drivers and passengers in the event of an accident. Cars also come with a variety of convenience features, such as power windows, power locks, and cruise control.\\n\\nOwning a car can be expensive. The cost of buying a car, as well as the cost of fuel, insurance, and maintenance can add up quickly. It is important to consider these costs when deciding whether or not to purchase a car.\\n\\nCars can also have a negative impact on the environment. The burning of fuel releases emissions into the atmosphere, which can contribute to air pollution. It is important to be aware of the environmental impact of cars and to take steps to reduce emissions when possible.\\n\\nCars are an incredibly important invention. They have revolutionized the way we travel and have made our lives easier. However, it is important to be aware of the costs and environmental impacts associated with owning a car.', \"\\n\\nCars have been a part of our lives for over a century now. They have changed the way we live, work, and travel. They have made our lives easier and more efficient. Cars have become an integral part of our society and our culture.\\n\\nCars have been around since the late 19th century, but it wasn't until the early 20th century that they began to be mass-produced and widely used. The first mass-produced car was the Ford Model T, which was released in 1908. The Model T was a huge success and sold over 15 million units in its lifetime. This was the start of the automotive revolution, and it changed the way people traveled and commuted.\\n\\nSince then, cars have become much more advanced and efficient. Automakers have developed cars that are more fuel-efficient, safer, and more reliable. This has made them much more accessible to the general public. Cars have also become much more affordable, making them more attainable for people of all income levels.\\n\\nCars have also had a huge impact on the environment. Automakers have developed more efficient engines and have started using more sustainable materials in the production of cars. This has led to a decrease in emissions and air pollution. Additionally, cars have become much more fuel-efficient, leading to a decrease in fuel consumption.\\n\\nCars have also had a huge impact on the economy. Automakers employ thousands of people and generate billions of dollars in revenue. This money is used to create jobs, fund research and development, and invest in new technologies. This has helped to create a strong and vibrant economy.\\n\\nCars have become an integral part of our lives. They have changed the way we live, work, and travel. They have made our lives easier and more efficient. They have also had a huge impact on the environment and the economy. Cars are here to stay, and they will continue to shape our lives for many years to come.\", '\\n\\nCars have been a part of everyday life for many decades now, and it’s hard to imagine a world without them. Cars are a symbol of freedom and independence, and they’ve come to represent the American Dream. But cars aren’t just a symbol of freedom, they’re also a big part of our economy and our lives.\\n\\nCars are a major part of our economy. In the United States, the automobile industry is one of the largest employers, providing jobs for millions of people. The auto industry also contributes billions of dollars to the GDP each year. Cars are also a major source of revenue for governments, as taxes and fees on cars make up a large portion of state and local budgets.\\n\\nCars are also a major part of our lives. They provide us with a way to get around, to go to work, to go shopping, to visit family and friends, and to go on vacation. Cars are also a way for us to express our own unique style and personality. We can customize our cars to fit our own preferences and tastes.\\n\\nBut cars also have a dark side. Cars produce air pollution, which can cause health problems for people, especially those with asthma or other respiratory illnesses. Cars also contribute to global warming, as they produce greenhouse gases that trap heat in the atmosphere. And, of course, cars are a major factor in traffic accidents, which can cause serious injuries and even death.\\n\\nDespite the dark side of cars, they remain an essential part of our lives. We rely on them to get around, to go to work, to visit family and friends, and to go on vacation. We customize them to fit our own preferences and tastes. We use them to express our own unique style and personality. And, of course, we use them to get to places we couldn’t otherwise get to. Cars are a major part of our economy and our lives, and they’re here to stay.']\n"
     ]
    }
   ],
   "source": [
    "# Set up the model and prompt\n",
    "model_engine = \"text-davinci-003\"\n",
    "prompt = \"\"\"Write a long essay about cars\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "# Generate a response\n",
    "completion = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=3500,\n",
    "    n=3,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "responses = [completion.choices[i].text for i in range(len(completion.choices))]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4d8c043f-e851-4a91-98c5-9c79e691274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Cars are one of the most important inventions of the modern age.', 'They have become an integral part of our lives, providing us with a convenient and comfortable way to travel.', 'Cars have revolutionized the way we travel, allowing us to go farther and faster than ever before.', 'Cars are powered by an internal combustion engine, which converts fuel into energy.', 'This energy is used to turn the wheels of the car, allowing it to move.', 'Cars come in a variety of sizes, shapes, and types, from small compact cars to large luxury vehicles.', 'They can be powered by gasoline, diesel, or electricity.', 'Cars are incredibly complex machines, with many different components and systems that work together to make them run.', 'The engine, transmission, brakes, suspension, and steering are all important components of a car.', 'Each of these systems must be in good condition for the car to run properly.', 'Cars also come with a variety of safety features, such as airbags and seat belts.', 'These safety features can help protect drivers and passengers in the event of an accident.', 'Cars also come with a variety of convenience features, such as power windows, power locks, and cruise control.', 'Owning a car can be expensive.', 'The cost of buying a car, as well as the cost of fuel, insurance, and maintenance can add up quickly.', 'It is important to consider these costs when deciding whether or not to purchase a car.', 'Cars can also have a negative impact on the environment.', 'The burning of fuel releases emissions into the atmosphere, which can contribute to air pollution.', 'It is important to be aware of the environmental impact of cars and to take steps to reduce emissions when possible.', 'Cars are an incredibly important invention.', 'They have revolutionized the way we travel and have made our lives easier.', 'However, it is important to be aware of the costs and environmental impacts associated with owning a car.'], ['Cars have been a part of our lives for over a century now.', 'They have changed the way we live, work, and travel.', 'They have made our lives easier and more efficient.', 'Cars have become an integral part of our society and our culture.', \"Cars have been around since the late 19th century, but it wasn't until the early 20th century that they began to be mass-produced and widely used.\", 'The first mass-produced car was the Ford Model T, which was released in 1908.', 'The Model T was a huge success and sold over 15 million units in its lifetime.', 'This was the start of the automotive revolution, and it changed the way people traveled and commuted.', 'Since then, cars have become much more advanced and efficient.', 'Automakers have developed cars that are more fuel-efficient, safer, and more reliable.', 'This has made them much more accessible to the general public.', 'Cars have also become much more affordable, making them more attainable for people of all income levels.', 'Cars have also had a huge impact on the environment.', 'Automakers have developed more efficient engines and have started using more sustainable materials in the production of cars.', 'This has led to a decrease in emissions and air pollution.', 'Additionally, cars have become much more fuel-efficient, leading to a decrease in fuel consumption.', 'Cars have also had a huge impact on the economy.', 'Automakers employ thousands of people and generate billions of dollars in revenue.', 'This money is used to create jobs, fund research and development, and invest in new technologies.', 'This has helped to create a strong and vibrant economy.', 'Cars have become an integral part of our lives.', 'They have changed the way we live, work, and travel.', 'They have made our lives easier and more efficient.', 'They have also had a huge impact on the environment and the economy.', 'Cars are here to stay, and they will continue to shape our lives for many years to come.'], ['Cars have been a part of everyday life for many decades now, and it’s hard to imagine a world without them.', 'Cars are a symbol of freedom and independence, and they’ve come to represent the American Dream.', 'But cars aren’t just a symbol of freedom, they’re also a big part of our economy and our lives.', 'Cars are a major part of our economy.', 'In the United States, the automobile industry is one of the largest employers, providing jobs for millions of people.', 'The auto industry also contributes billions of dollars to the GDP each year.', 'Cars are also a major source of revenue for governments, as taxes and fees on cars make up a large portion of state and local budgets.', 'Cars are also a major part of our lives.', 'They provide us with a way to get around, to go to work, to go shopping, to visit family and friends, and to go on vacation.', 'Cars are also a way for us to express our own unique style and personality.', 'We can customize our cars to fit our own preferences and tastes.', 'But cars also have a dark side.', 'Cars produce air pollution, which can cause health problems for people, especially those with asthma or other respiratory illnesses.', 'Cars also contribute to global warming, as they produce greenhouse gases that trap heat in the atmosphere.', 'And, of course, cars are a major factor in traffic accidents, which can cause serious injuries and even death.', 'Despite the dark side of cars, they remain an essential part of our lives.', 'We rely on them to get around, to go to work, to visit family and friends, and to go on vacation.', 'We customize them to fit our own preferences and tastes.', 'We use them to express our own unique style and personality.', 'And, of course, we use them to get to places we couldn’t otherwise get to.', 'Cars are a major part of our economy and our lives, and they’re here to stay.']]\n"
     ]
    }
   ],
   "source": [
    "essays = [i.split(\"\\n\") for i in responses]\n",
    "sentences = []\n",
    "for essay_li in essays:\n",
    "    essay_sents = []\n",
    "    for portion in essay_li:\n",
    "        if len(portion.strip()) == 0:\n",
    "            continue\n",
    "        add_li = re.split('(?<=[.!?]) +',str(portion))\n",
    "        essay_sents += add_li\n",
    "        #print(essay_sents)\n",
    "    sentences.append(essay_sents)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e043be-f86d-40aa-b7c0-2e9e4d4bdcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
